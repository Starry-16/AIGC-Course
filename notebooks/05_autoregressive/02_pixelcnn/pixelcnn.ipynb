{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "206a93a2-e9b0-4ea2-a43f-696faa83ea03",
   "metadata": {},
   "source": [
    "# ðŸ‘¾ PixelCNN from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af9e216-7e84-4f5b-a2db-26aca3bea464",
   "metadata": {},
   "source": [
    "In this notebook, we'll walk through the steps required to train your own PixelCNN on the fashion MNIST dataset from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1e6bbc-6f3b-48ac-a4f3-fde6f739f0ca",
   "metadata": {},
   "source": [
    "The code has been adapted from the excellent [PixelCNN tutorial](https://keras.io/examples/generative/pixelcnn/) created by ADMoreau, available on the Keras website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6acebfa8-4546-41fd-adaa-2307c65b1b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, optimizers, callbacks\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"..\")))\n",
    "from notebooks.utils import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8543166d-f4c7-43f8-a452-21ccbf2a0496",
   "metadata": {},
   "source": [
    "## 0. Parameters <a name=\"parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "444d84de-2843-40d6-8e2e-93691a5393ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 16\n",
    "PIXEL_LEVELS = 4\n",
    "N_FILTERS = 128\n",
    "RESIDUAL_BLOCKS = 5\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65dac68-d20b-4ed9-a136-eed57095ce4f",
   "metadata": {},
   "source": [
    "## 1. Prepare the data <a name=\"prepare\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ed0fc56-d1b0-4d42-b029-f4198f78e666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 12us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 4s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 2s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "(x_train, _), (_, _) = datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b667e78c-8fa7-4e5b-a2c0-69e50166ef77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-12 23:45:24.191087: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2026-02-12 23:45:24.191161: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2026-02-12 23:45:24.191209: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2026-02-12 23:45:24.191255: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2026-02-12 23:45:24.229515: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2026-02-12 23:45:24.229701: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2026-02-12 23:45:24.230558: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "def preprocess(imgs_int):\n",
    "    imgs_int = np.expand_dims(imgs_int, -1)\n",
    "    imgs_int = tf.image.resize(imgs_int, (IMAGE_SIZE, IMAGE_SIZE)).numpy()\n",
    "    imgs_int = (imgs_int / (256 / PIXEL_LEVELS)).astype(int)\n",
    "    imgs = imgs_int.astype(\"float32\")\n",
    "    imgs = imgs / PIXEL_LEVELS\n",
    "    return imgs, imgs_int\n",
    "\n",
    "\n",
    "input_data, output_data = preprocess(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3c2b304-8385-4931-8291-9b7cc462c95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x300 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show some items of clothing from the training set\n",
    "display(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccd5cb2-8c7b-4667-8adb-4902f3fa60cf",
   "metadata": {},
   "source": [
    "## 2. Build the PixelCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "847050a5-e4e6-4134-9bfc-c690cb8cb44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first layer is the PixelCNN layer. This layer simply\n",
    "# builds on the 2D convolutional layer, but includes masking.\n",
    "class MaskedConv2D(layers.Layer):\n",
    "    def __init__(self, mask_type, **kwargs):\n",
    "        super(MaskedConv2D, self).__init__()\n",
    "        self.mask_type = mask_type\n",
    "        self.conv = layers.Conv2D(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Build the conv2d layer to initialize kernel variables\n",
    "        self.conv.build(input_shape)\n",
    "        # Use the initialized kernel to create the mask\n",
    "        kernel_shape = self.conv.kernel.get_shape()\n",
    "        self.mask = np.zeros(shape=kernel_shape)\n",
    "        self.mask[: kernel_shape[0] // 2, ...] = 1.0\n",
    "        self.mask[kernel_shape[0] // 2, : kernel_shape[1] // 2, ...] = 1.0\n",
    "        if self.mask_type == \"B\":\n",
    "            self.mask[kernel_shape[0] // 2, kernel_shape[1] // 2, ...] = 1.0\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.conv.kernel.assign(self.conv.kernel * self.mask)\n",
    "        return self.conv(inputs)\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a52f7795-790e-47b0-b724-80be3e3c3666",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(layers.Layer):\n",
    "    def __init__(self, filters, **kwargs):\n",
    "        super(ResidualBlock, self).__init__(**kwargs)\n",
    "        self.conv1 = layers.Conv2D(\n",
    "            filters=filters // 2, kernel_size=1, activation=\"relu\"\n",
    "        )\n",
    "        self.pixel_conv = MaskedConv2D(\n",
    "            mask_type=\"B\",\n",
    "            filters=filters // 2,\n",
    "            kernel_size=3,\n",
    "            activation=\"relu\",\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.conv2 = layers.Conv2D(\n",
    "            filters=filters, kernel_size=1, activation=\"relu\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.pixel_conv(x)\n",
    "        x = self.conv2(x)\n",
    "        return layers.add([inputs, x])\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19b4508f-84de-42a9-a77f-950fb493db13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 16, 16, 1)]       0         \n",
      "                                                                 \n",
      " masked_conv2d (MaskedConv2D  (None, 16, 16, 128)      6400      \n",
      " )                                                               \n",
      "                                                                 \n",
      " residual_block (ResidualBlo  (None, 16, 16, 128)      53504     \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " residual_block_1 (ResidualB  (None, 16, 16, 128)      53504     \n",
      " lock)                                                           \n",
      "                                                                 \n",
      " residual_block_2 (ResidualB  (None, 16, 16, 128)      53504     \n",
      " lock)                                                           \n",
      "                                                                 \n",
      " residual_block_3 (ResidualB  (None, 16, 16, 128)      53504     \n",
      " lock)                                                           \n",
      "                                                                 \n",
      " residual_block_4 (ResidualB  (None, 16, 16, 128)      53504     \n",
      " lock)                                                           \n",
      "                                                                 \n",
      " masked_conv2d_6 (MaskedConv  (None, 16, 16, 128)      16512     \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " masked_conv2d_7 (MaskedConv  (None, 16, 16, 128)      16512     \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 16, 16, 4)         516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 307,460\n",
      "Trainable params: 307,460\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "x = MaskedConv2D(\n",
    "    mask_type=\"A\",\n",
    "    filters=N_FILTERS,\n",
    "    kernel_size=7,\n",
    "    activation=\"relu\",\n",
    "    padding=\"same\",\n",
    ")(inputs)\n",
    "\n",
    "for _ in range(RESIDUAL_BLOCKS):\n",
    "    x = ResidualBlock(filters=N_FILTERS)(x)\n",
    "\n",
    "for _ in range(2):\n",
    "    x = MaskedConv2D(\n",
    "        mask_type=\"B\",\n",
    "        filters=N_FILTERS,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        activation=\"relu\",\n",
    "        padding=\"valid\",\n",
    "    )(x)\n",
    "\n",
    "out = layers.Conv2D(\n",
    "    filters=PIXEL_LEVELS,\n",
    "    kernel_size=1,\n",
    "    strides=1,\n",
    "    activation=\"softmax\",\n",
    "    padding=\"valid\",\n",
    ")(x)\n",
    "\n",
    "pixel_cnn = models.Model(inputs, out)\n",
    "pixel_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442b5ffa-67a3-4b15-a342-eb1eed5e87ac",
   "metadata": {},
   "source": [
    "## 3. Train the PixelCNN <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7204789a-2ad3-48bf-b7e8-00d4cab10d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(learning_rate=0.0005)\n",
    "pixel_cnn.compile(optimizer=adam, loss=\"sparse_categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09d327fc-aff8-40e6-b390-d1bff4c06ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "\n",
    "\n",
    "class ImageGenerator(callbacks.Callback):\n",
    "    def __init__(self, num_img):\n",
    "        self.num_img = num_img\n",
    "\n",
    "    def sample_from(self, probs, temperature):  # <2>\n",
    "        probs = probs ** (1 / temperature)\n",
    "        probs = probs / np.sum(probs)\n",
    "        return np.random.choice(len(probs), p=probs)\n",
    "\n",
    "    def generate(self, temperature):\n",
    "        generated_images = np.zeros(\n",
    "            shape=(self.num_img,) + (pixel_cnn.input_shape)[1:]\n",
    "        )\n",
    "        batch, rows, cols, channels = generated_images.shape\n",
    "\n",
    "        for row in range(rows):\n",
    "            for col in range(cols):\n",
    "                for channel in range(channels):\n",
    "                    probs = self.model.predict(generated_images, verbose=0)[\n",
    "                        :, row, col, :\n",
    "                    ]\n",
    "                    generated_images[:, row, col, channel] = [\n",
    "                        self.sample_from(x, temperature) for x in probs\n",
    "                    ]\n",
    "                    generated_images[:, row, col, channel] /= PIXEL_LEVELS\n",
    "\n",
    "        return generated_images\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        generated_images = self.generate(temperature=1.0)\n",
    "        display(\n",
    "            generated_images,\n",
    "            save_to=\"./output/generated_img_%03d.png\" % (epoch),\n",
    "        )\n",
    "\n",
    "\n",
    "img_generator_callback = ImageGenerator(num_img=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85231056-d4a4-4897-ab91-065325a18d93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "414/469 [=========================>....] - ETA: 13s - loss: 0.5037"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpixel_cnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_generator_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/aigc_tensor/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/aigc_tensor/lib/python3.10/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/aigc_tensor/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/aigc_tensor/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/aigc_tensor/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/aigc_tensor/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/aigc_tensor/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/aigc_tensor/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/aigc_tensor/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pixel_cnn.fit(\n",
    "    input_data,\n",
    "    output_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[tensorboard_callback, img_generator_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb4fa72-dd2d-44c1-ad18-9c965060683e",
   "metadata": {},
   "source": [
    "## 4. Generate images <a name=\"generate\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbd4643-be09-49ba-b7bc-a524a2f00806",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = img_generator_callback.generate(temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cadb4b-ae2c-42a9-92ac-68e2131380ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(generated_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aigc_tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
